{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuroPop:\n",
    "    \"\"\"\n",
    "    This class implements several conveniences for \n",
    "    plotting, fitting and decoding from population tuning curves\n",
    "    \n",
    "    We assume that neurons have a tuning curve of the form:\n",
    "    f(x) = b_ + g_ * exp(k_ * cos(x - mu_))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: float, n_samples x 1, feature of interest\n",
    "    Y: float, n_samples x n_neurons, population activity\n",
    "    mu_: float,  n_neurons x 1, preferred feature [-pi, pi]\n",
    "    k_: float,  n_neurons x 1, shape (width)\n",
    "    g_: float,  n_neurons x 1, gain\n",
    "    b_: float,  n_neurons x 1, baseline\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    encode\n",
    "    tunefit\n",
    "    display\n",
    "    decode\n",
    "\n",
    "    Other helper methods\n",
    "    --------------------\n",
    "    vonmises\n",
    "    loss\n",
    "    grad_loss\n",
    "    jointlogL    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, Y):\n",
    "        \"\"\"\n",
    "        Initialize the object\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.Y = Y\n",
    "        (n_samples, n_neurons) = Y.shape\n",
    "        self.n_neurons = n_neurons\n",
    "        # Assign random parameters\n",
    "        \n",
    "        self.mu_ = np.pi*(2.0*np.random.rand(n_neurons) - 1.0)\n",
    "        self.k_ = 50.0*np.random.rand(n_neurons)\n",
    "        self.g_ = 5.0*np.random.rand(n_neurons)\n",
    "        self.b_ = 10.0*np.random.rand(n_neurons)\n",
    "        \n",
    "    #-----------------------------------------------------------------------\n",
    "    def vonmises(x, mu, k, g, b):\n",
    "        \"\"\"\n",
    "        The von Mises tuning function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: float, n_samples x 1, feature of interest\n",
    "        mu: float,  n_neurons x 1, preferred feature [-pi, pi]\n",
    "        k: float,  n_neurons x 1, shape (width)\n",
    "        g: float,  n_neurons x 1, gain\n",
    "        b: float,  n_neurons x 1, baseline\n",
    "        \n",
    "        Outputs\n",
    "        -------\n",
    "        Y: float, n_samples x 1, firing rates\n",
    "        \"\"\"\n",
    "        Y = b + g * np.exp (k * cos(x - mu))\n",
    "        return Y\n",
    "    \n",
    "    #-----------------------------------------------------------------------\n",
    "    def loss(x, y, mu, k, g, b):\n",
    "        \"\"\"\n",
    "        The loss function: negative Poisson log likelihood function \n",
    "        under the von mises tuning model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: float, n_samples x 1, feature of interest\n",
    "        y: float, n_samples x 1, firing rates\n",
    "        mu: float,  n_neurons x 1, preferred feature [-pi, pi]\n",
    "        k: float,  n_neurons x 1, shape (width)\n",
    "        g: float,  n_neurons x 1, gain\n",
    "        b: float,  n_neurons x 1, baseline\n",
    "        \n",
    "        Outputs\n",
    "        -------\n",
    "        loss: float, scalar\n",
    "        \"\"\"\n",
    "        lmb = b + g * np.exp (k * cos(x - mu))\n",
    "        loss = np.sum(lmb) - np.sum(y * lmb)\n",
    "        return loss\n",
    "    \n",
    "    #-----------------------------------------------------------------------\n",
    "    def grad_loss(x, y, mu, k, g, b):\n",
    "        \"\"\"\n",
    "        The gradient of the loss function:\n",
    "        wrt parameters of the von mises tuning model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: float, n_samples x 1, feature of interest\n",
    "        y: float, n_samples x 1, firing rates\n",
    "        mu: float,  n_neurons x 1, preferred feature [-pi, pi]\n",
    "        k: float,  n_neurons x 1, shape (width)\n",
    "        g: float,  n_neurons x 1, gain\n",
    "        b: float,  n_neurons x 1, baseline\n",
    "        \n",
    "        Outputs\n",
    "        -------\n",
    "        grad_mu: float, scalar\n",
    "        grad_k: float, scalar\n",
    "        grad_g: float, scalar\n",
    "        grad_b: float, scalar\n",
    "        \"\"\"\n",
    "        \n",
    "        lmb = b + g * np.exp (k * cos(x - mu))\n",
    "        grad_mu = np.sum(g * np.exp(k * np.cos(x - mu)) * k * np.sin(x - mu) * (1 - y/lmb))\n",
    "        grad_k = np.sum(g * np.exp(k * np.cos(x - mu)) * np.cos(x - mu) * (1 - y/lmb))\n",
    "        grad_g = np.sum(g * np.exp(k * np.cos(x - mu)) * (1 - y/lmb))\n",
    "        grad_b = np.sum((1-y/lmb))\n",
    "        return grad_mu, grad_k, grad_g, grad_b\n",
    "    \n",
    "    #-----------------------------------------------------------------------\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Compute the firing rates for the population \n",
    "        based on the von Mises tuning models\n",
    "        given features\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: float, n_samples x 1, feature of interest\n",
    "        \n",
    "        Outputs\n",
    "        -------\n",
    "        Y: float, n_samples x n_neurons, population activity\n",
    "        \"\"\"\n",
    "        # For each neuron\n",
    "        for n in len(self.n_neurons):\n",
    "            # Compute the firing rate under the von Mises model\n",
    "            Y[:, n] = vonmises(x, self.mu_[n], self.k_[n], self.g_[n], self.b_[n])\n",
    "        return Y\n",
    "    \n",
    "    #-----------------------------------------------------------------------\n",
    "    def tunefit(self):\n",
    "        \"\"\"\n",
    "        Estimate the parameters of the tuning curve under the \n",
    "        von Mises model, given features and population activity\n",
    "        \"\"\"\n",
    "    \n",
    "    #-----------------------------------------------------------------------\n",
    "    def decode(self):\n",
    "        \"\"\"\n",
    "        Estimate the parameters of the tuning curve under the \n",
    "        von Mises model, given features and population activity\n",
    "        \"\"\"\n",
    "        \n",
    "    #-----------------------------------------------------------------------\n",
    "    def display(self):\n",
    "        \"\"\"\n",
    "        Estimate the parameters of the tuning curve under the \n",
    "        von Mises model, given features and population activity\n",
    "        \"\"\"\n",
    "        #for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting tuning curves with gradient descent\n",
    "\n",
    "The firing rates $y$ can be modeled as a Poisson random variable. \n",
    "\n",
    "$$\n",
    "y = \\text{Poisson}(\\lambda)\n",
    "$$\n",
    "\n",
    "The mean $\\lambda$ is given by the von Mises tuning model as follows.\n",
    "\n",
    "$$\n",
    "\\lambda = b + g\\exp\\Big(\\kappa \\cos(x - \\mu)\\Big)\n",
    "$$\n",
    "\n",
    "Given a set of observations $(x_i, y_i)$, to identify the parameters $\\Theta = \\left\\{\\mu, \\kappa, g, b\\right\\}$ we use gradient descent on the loss function $J$, specified by the negative log-likelihood,\n",
    "\n",
    "$$\n",
    "J = -\\log\\mathcal{L} = \\sum_{i} \\lambda_i - y_i \\log \\lambda_i\n",
    "$$\n",
    "\n",
    "Taking the gradients, we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mu} = \\sum_{i} g \\exp\\Big(\\kappa \\cos(x_i - \\mu)\\Big) \\kappa \\sin(x_i - \\mu)\\bigg(1 - \\frac{y_i}{\\lambda_i}\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\kappa} = \\sum_{i} g \\exp\\Big(\\kappa \\cos(x_i - \\mu)\\Big) \\cos(x_i - \\mu)\\bigg(1 - \\frac{y_i}{\\lambda_i}\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial g} = \\sum_{i} g \\exp\\Big(\\kappa \\cos(x_i - \\mu)\\Big) \\bigg(1 - \\frac{y_i}{\\lambda_i}\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\sum_{i} \\bigg(1 - \\frac{y_i}{\\lambda_i}\\bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.05      ,  0.14691943])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([12.1, 3.1])\n",
    "b = np.array([2.0, 21.1])\n",
    "a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
